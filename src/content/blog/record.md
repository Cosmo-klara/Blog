---
title: '一些学习记录'
publishDate: '2025-10-15'
updatedDate: '2025-10-22'
description: '学习中看到的很有趣的说法'
tags:
    - Record
language: '中文'
---

# 一些学习记录

## Embedding

**Embedding 就是让计算机能够"理解"离散数据的含义, 并用数学向量的形式表达这种理解。**

### 为什么需要 Embedding？

原始表示的局限性

```python
# One-hot 编码（传统方法）
"猫" = [1, 0, 0, 0, 0]
"狗" = [0, 1, 0, 0, 0] 
"汽车" = [0, 0, 1, 0, 0]

# 问题：
# 1. 维度灾难（词汇表很大时）
# 2. 所有词之间的距离相等
# 3. 无法表达语义关系
```

解决方案：Embedding

```python
# Embedding 表示
"猫" = [0.8, 0.1, 0.9]
"狗" = [0.7, 0.2, 0.8]      # 与猫相似
"汽车" = [0.1, 0.9, 0.2]    # 与猫狗差异大
"老虎" = [0.75, 0.15, 0.85] # 与猫更相似
```

### Embedding 的数学特性

语义关系保持：

```python
# 例如：
"国王" → [0.2, 0.8, -0.3, 0.5]
"皇后" → [0.3, 0.7, -0.2, 0.6]
"男人" → [0.1, 0.9, -0.4, 0.4]
"女人" → [0.2, 0.8, -0.3, 0.5]

# 向量("国王") - 向量("男人") + 向量("女人") ≈ 向量("皇后")
```

## 全监督, 自监督, 半监督, 弱监督, 无监督的关系和区别

### 1. 全监督学习（supervised learning）

    训练数据都有**完整和准确**的标签, 模型通过学习输入数据和标签之间的映射关系来完成特定的任务, 例如分类、回归等。

    特点：
    1. 需要大量的标注数据。
    2. 模型的目标是学习输入特征和输出标签之间的映射关系。

### 2. 无监督学习（unsupervised learning）

    训练数据**没有任何标签**, 模型通过学习数据本身的分布或结构来发现一些隐含的模式或特征, 例如聚类、降维等。

    特点：
    1. 没有任何标注数据或监督信号。
    2. 旨在发现数据的内在结构, 如聚类结构或低维表示。目标是学习数据的分布或模式, 而不是预测输出。

### 3. 半监督学习（semi-supervised learning）

    训练数据既有一**部分有标签**的数据, 也有一部分没有标签的数据, 模型通过结合两种数据来提高学习性能, 例如利用有标签数据训练一个教师模型, 然后用教师模型给无标签数据生成**伪标签**, 再用伪标签训练一个学生模型。

    特点：
    1. 数据集包含少量标注数据和大量未标注数据。
    2. 利用数据的内在结构（如聚类结构）来辅助学习过程。


### 4. 自监督学习（self-supervised learning）

    训练数据没有外部给定的标签, 但是模型可以通过一些辅助任务来自动生成一些内部标签（伪标签）, 然后用这些内部标签来训练模型, 从而学习到数据的内在表示, 再用这些表示来做 downstream, 例如在图像上做旋转预测或遮挡恢复等辅助任务, 然后用得到的特征向量来做分类或检测等下游任务。

    特点：
    1. 无需标注数据。
    2. 通过设计特定的预训练任务生成伪标签。
    3. 学习到的表示可以迁移到各种下游任务中。

### 5. 弱监督学习（weakly supervised learning）

    训练数据有一些**不完整、不确切或不准确**的标签, 模型通过学习这些低质量的标签来完成一个更困难的任务, 例如利用图片级别的分类标签来做目标检测或分割。

    弱监督学习可以克服标注难度高或噪声大的问题, 提高模型泛化能力。







